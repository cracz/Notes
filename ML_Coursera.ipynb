{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning from Coursera by Andrew Ng\n",
    "\n",
    "---\n",
    "\n",
    "## Octave\n",
    " \n",
    " Useful for prototyping ML projects because it has ways to easily implement ML algorithms with builtin functions.\n",
    "\n",
    "--\n",
    " \n",
    "Examples of Machine Learning:\n",
    "\n",
    "* Database mining (Web click data, medical records, biology, engineering)\n",
    "* Applications that can't be programmed by hand (Autonomous helicopter, handwriting recognition, most of Natural Language Processing (NLP), computer vision.\n",
    "* Self-customizing programs (Amazon, Netflix product recommendations)\n",
    "\n",
    "## Machine Learning Algorithms\n",
    " \n",
    "### Supervised Learning\n",
    " \n",
    "* The \"correct answers\" are given for the algorithm to learn from; A.K.A. the \"training set\".\n",
    "* Usually useful for __Regression__: Predicting a continuous valued output.\n",
    "* __Classification__ problems: predicting a discrete valued output, like 0 or 1 (no or yes), specific categories, etc.\n",
    "* Notation for this course: $m$ = number of training examples, $x$ = input variable (features), $y$ = output variable (target variable), $(x,y)$ = single training example.\n",
    " \n",
    " \n",
    "### Unsupervised Learning\n",
    " \n",
    "* The \"correct answers\" are not given and the algorithm has to learn some structure in the data.\n",
    "* A common thing for the algorithms to do is to __cluster__ the data into separate groups. (i.e. Google News, human genes, organizing computing clusters, social network analysis, market segmentation, astronomical data analysis).\n",
    " \n",
    " \n",
    "### Univariate Linear Regression\n",
    " \n",
    "* One variable, model $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ is a straight line mapping $x$ to $y$ after fitting to the $m$ training examples.\n",
    "* Fitting process:\n",
    " * Find $\\theta_0$ and $\\theta_1$ in $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ such that we minimize the __cost function__ (or __squared error function__):\n",
    " \\begin{equation*}\n",
    "    J(\\theta_0, \\theta_1) = \\frac{1}{2m} * \\Sigma_{i=1}^m (h_{\\theta}(x_i)-y_i)^2\n",
    " \\end{equation*}\n",
    "\n",
    " \n",
    " ### Univariate Gradient Descent\n",
    "\n",
    " * Basic idea is to start with some $\\theta_0$, $\\theta_1$ and keep changing them to reduce $J(\\theta_0, \\theta_1)$ until we end up at a minimum.\n",
    " * This can be applied for any number of variables $\\theta_0,\\dots,\\theta_n$.\n",
    " * For each variable $\\theta_j$, repeatedly update $\\theta_j$ with $\\theta_j-(\\delta J/\\delta\\theta_j)*\\alpha$ (where $\\alpha$ is the __learning rate__).\n",
    " * You must simultaneously update all $\\theta_j$; don't do it one at a time because each update will affect the derivative for the other variables.\n",
    "\n",
    "\n",
    "### Multivariate Linear Regression\n",
    "\n",
    "* When there's more than one feature (independent variable) involved, the hypothesis becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "    h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\dots + \\theta_n x_n\n",
    "\\end{equation*}\n",
    "\n",
    "* Notation:\n",
    " * $x^{(i)} =$ the vector of features of the $i^{th}$ training example\n",
    " * $x_j^{(i)} =$ value of feature $j$ in the $i^{th}$ training example\n",
    " * $m = $ the number of training examples\n",
    " * $n = $ the number of features\n",
    "\n",
    "* Let $x_0^{(i)} = 1$ for all $i$. Then we can just do vector/matrix multiplication with two $n+1$ dimensional vectors $\\theta$ and $x$ so that \n",
    "\n",
    "\\begin{equation*}\n",
    "    h_{\\theta}(x) = \\theta^{T}x\n",
    "\\end{equation*}\n",
    "\n",
    "### Multivariate Gradient Descent\n",
    " \n",
    " * Use the same cost function, but now you have vectors $\\theta$, $x^{(i)}$, and $y^{(i)}$, and the derivatives are with respect to each $\\theta_j$:\n",
    "\n",
    " \\begin{equation*}\n",
    "    J(\\theta) = \\frac{1}{2m} * \\Sigma_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})^2 \\\\\n",
    "    \\theta_j = \\theta_j - \\alpha\\frac{\\delta J(\\theta)}{\\delta\\theta_j} \\textrm{ simultaneously for all } j = 0,\\dots,n\n",
    " \\end{equation*}\n",
    "\n",
    " * __Feature Scaling__: Make sure features are on a similar scale. This can help gradient descent converge quickly. Get every feature into approximately a $-1 \\leq x_i \\leq 1$ range. This is not an exact requirement, but is very useful.\n",
    "\n",
    " * __Mean Normalization__: Replace $x_i$ with $x_i-\\mu_i$ (where $\\mu_i$ is the average of $x_i$) to make features have approximately zero mean (do not apply to $x_0=1$). Or replace with $(x_i-\\mu_i)/s_i$, where $s_i$ is either the range of $x_i$ values or the standard deviation.\n",
    "\n",
    " * __Learning Rate__: Plot $J(\\theta)$ as gradient descent runs vs. number of iterations. Flattening out shows roughly how many iterations are necessary to converge. (example auto convergence test: declare convergence if $J(\\theta)$ decreases by less that $10^{-3}$ in one iteration.) If $J(\\theta)$ is increasing, or showing periodic behavior, use a smaller $\\alpha$. For sufficiently small $\\alpha$, gradient descent should decrease on __every__ iteration. An $\\alpha$ that is too small will make gradient descent very slow though. Maybe try 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.1.\n",
    "\n",
    "### Features and Polynomial Regression\n",
    "\n",
    " * Rather than using an equation like $h_{\\theta}(x)=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3$, you may be able to choose a different feature $x$ that combines $x_1, x_2, x_3$ in some way so that you end up with a polynomial like $h_{\\theta}(x)=\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$. E.g. instead of using width and height, use area. The regression process is the same as before, but due to the exponents, feature scaling will become much more important.\n",
    "\n",
    "### Normal Equation\n",
    "\n",
    " * For a cost function $J(\\theta)$, the normal equation gives us a method to solve for $\\theta$ analytically, rather than using an iterative method like in gradient descent.\n",
    "\n",
    " * Usually to minimize a function $J(\\theta)$, we solve the equation $dJ(\\theta)/d\\theta=0$ for $\\theta$. If $\\theta$ is an $n+1$ dimensional vector, we solve the series of equations $\\delta J(\\theta)/\\delta\\theta_j=0$ for every $j$ to find $\\theta_0,\\theta_1,\\dots,\\theta_n$ that minimizes $J(\\theta)$.\n",
    "\n",
    " * for some dataset with $m$ training examples, and $n$ features ($x_1, x_2, \\dots, x_n$), we can add an $x_0$ feature of all 1's, and form a ``design matrix\" $X$ out of the values of all of the features. Each __row__ of $X$ is a training example, not columns. This will be an $m$ x $n+1$ matrix. Also we'll take the output values $y$ you're trying to predict and form an $m$-dimensional vector $Y$. The $\\theta$ that minimizes the cost function is given by $\\theta=(X^TX)^{-1}X^T Y$.\n",
    "\n",
    " * With this method, feature scaling is not necessary.\n",
    "\n",
    " * No need to choose $\\alpha$, don't need to iterate, however we need to compute $(X^TX)^{-1}$, an $n$ x $n$ matrix, and this makes the method slow for large $n$. The time to invert $X^TX$ grows like $O(n^3)$.\n",
    "\n",
    " * Maybe switch to gradient descent if $n\\sim 10^6$.\n",
    "\n",
    " * If $X^TX$ is non-invertible (singular or degenerate), it's usually because there are redundant features ($x_1=$ size in feet$^2$, $x_2=$ size in meters$^2$), or too many features ($m\\leq n$). If this invertibility problem arises, look at the features and get rid of redundant ones or use less features if there were too many, or use regularization.\n",
    "\n",
    "### Octave Tutorial\n",
    "\n",
    "* __Basic Operations__\n",
    "\n",
    " * Not equal is ~= and not !=. Pi is just given by \"pi\".\n",
    "\n",
    " * __Single quotes__ to represent strings.\n",
    "\n",
    " * You can use the function __disp()__ to display things in the prompt, and use __sprintf()__ just like C __printf()__ to show strings with numbers.\n",
    "\n",
    " * __Operations on or with matrices__ will automatically convert the necessary things like constants into matrices to properly perform the operations.\n",
    "\n",
    " * __Create a matrix__ with square brackets and separate rows by semicolons. 3x2 matrix is A = [1 2; 3 4; 5 6]. To make a vector from 1 to 2 with step size of 0.1 between each element, use v = 1:0.1:2 (not semicolons). Or with step size of 1, just omit the middle number like v = 1:6.\n",
    "\n",
    " * To make a matrix of all ones, use the __function ones(rows, columns)__. Other functions include __zeros(rows, columns)__, __rand(rows, columns)__ that fills with random numbers between 0 and 1, and __randn(rows, columns)__ which uses Gaussian random numbers with mean = 0 and sigma = 1. \n",
    "\n",
    " * The function __hist(vector)__ plots a histogram with the data from the given vector. Or __hist(vector, bins)__ to get control of the number of bins.\n",
    "\n",
    " * __eye(n)__ gives an $n$x$n$ identity matrix.\n",
    "\n",
    " * Use __help functionName__ to get info on a particular function.\n",
    "\n",
    "* __Moving Data Around__\n",
    "\n",
    " * Use __normal Unix command line__ commands to change/list directories, etc.\n",
    "\n",
    " * __load file.dat__ to load a certain text file called 'file.dat' that has training examples in rows and features/prediction values in columns. Make separate files for features and prediction values.\n",
    "\n",
    " * __who__ shows what variables are currently stored, __whos__ gives more details on the variables. __clear__ erases a variable you give it, or all variables if you don't give it any arguments.\n",
    "\n",
    " * Select first 10 elements of some vector v with __v(1:10)__.\n",
    "\n",
    " * Use __save file.mat variable__ to save some variable like a matrix to a MATLAB type file called 'file.mat'. In order to save as human-readable text file, use __save file.txt variable -ascii__.\n",
    "\n",
    " * __size(matrix)__ returns a 1x2 vector containing the number of rows and number of columns. __size(matrix,1)__ gives just first element (rows), __size(matrix,2)__ gives just the second element (columns).\n",
    "\n",
    " * __length(matrix)__ returns the length of the longest dimension of the matrix. If it's a vector it'll just give you the length.\n",
    "\n",
    " * __Selecting elements:__ Retrieve the a,b element of some matrix A: __A(a,b)__. Retrieve everything in the ath row or bth column, use a colon like __A(a,:)__ or __A(:,b)__. The resulting vector will still be a row or column vector like it was in the matrix. To retrieve everything from the first and third rows at the same time, use __A([1 3],:)__. Use these same operations for assigning values within the matrix.\n",
    "\n",
    " * To __append a new column__ to some matrix A, use __A = [A, [COLUMN VECTOR]]__. Putting two matrices A and B together is just __[A, B]__ (left and right) or __[A; B]__ (top and bottom).\n",
    "\n",
    " * __A(:)__ returns all the elements of matrix A rearranged into a 1-D vector.\n",
    "\n",
    "* __Computing on Data__\n",
    "\n",
    " * __Multiplying__: Normal is just __A\\*B__, or if you just want each element in A multiplied by it's corresponding element in B (element-wise), use __A.\\*B__ . The period is usually used to denote __element-wise operations__, for example changing each element of A into their inverse fraction is 1 ./ A . __log(A)__ or __exp(A)__ will also be element-wise operations within the matrix A.\n",
    "\n",
    " * __Transpose__ a matrix A with a single quote like A'.\n",
    "\n",
    " * To get the __max element value and index__ of that element within A, use [val, ind] = max(A). Now val = max value and ind = the index of that value.\n",
    "\n",
    " * __Element-wise comparisons__ can be done to get a vector/matrix of booleans. A < 3 will return 0 or 1 in each element depending on the condition. __find(A < 3)__ returns the index of all elements that pass the condition. \n",
    "\n",
    " * For __rounding__ numbers, use __floor()__ or __ceil()__.\n",
    "\n",
    " * __Element-wise maximum of two matrices__ A and B is __max(A, B)__. __Column-wise max__ of matrix A is __max(A,[],1)__ or just default output of __max(A)__, and __Row-wise max__ is __max(A,[],2)__. Single max element of A is __max(max(A))__ or __max(A(:))__.\n",
    "\n",
    " * __Matrix inverse__ is __pinv(A)__. A.K.A. pseudo-inverse.\n",
    "\n",
    "* __Plotting Data__\n",
    "\n",
    " * With some vectors x and y, you can plot these with __plot(x,y)__. Red color with __plot(x,y,'r')__. __Superimpose__ two by plotting one, issuing the command __hold on__ and then plotting the second. Use __close__ command to get rid of the plot.\n",
    "\n",
    " * __Multiple separate plots__ can be produced by using the __figure(n)__ command first before issuing __plot()__. This starts a figure numbered as 'n' so you can have different windows for figure 1, figure 2, etc.\n",
    "\n",
    " * __Multiple plots in the same window__ would be started with __subplot(a,b,c)__. This divides the window into an 'a x b' grid and starts with accessing subplot 'c'. After filling subplot 'c', access the next subplot spot by the command __subplot(a,b,c+1)__. Same as before but with a different third argument, it doesn't actually subdivide again.\n",
    "\n",
    " * Set __labels__ with __xlabel()__ and __ylabel()__, __title()__ and __legend()__.\n",
    "\n",
    " * Changing the __axis ranges__ is done by the command __axis([x1 x2 y1 y2])__.\n",
    "\n",
    " * __Saving as a PNG__ would be __print -dpng 'name.png'__.\n",
    "\n",
    " * __Clear a figure__ with __clf__.\n",
    "\n",
    " * __Visualize a matrix__ A as a sort of \"2-D histogram\" sort of thing with __imagesc(A)__. Or to show the z-axis scale you can run __imagesc(A), colorbar__.\n",
    "\n",
    "* __Control Statements__\n",
    "\n",
    " * __For Loops__: Loop from 1 to 10 with __for i=1:10, statements in the loop that do stuff; end;__ . So make sure __i=someRange,__ with a comma, then end the other stuff with semicolons and put an __end;__ at the end of the loop.\n",
    "\n",
    " * __break__ and __continue__ still work as normal.\n",
    "\n",
    " * __While Loops__: Pretty much same as above, __while i<=5, statements; end;__\n",
    "\n",
    " * __If Statements__: Look the same as above as well, make sure it also has an __end;__ statement. __if i==6, statements; end;__ . __elseif__ and __else__ are also possible.\n",
    "\n",
    " * __Defining Functions__: Functions need to be saved in a MATLAB file like 'myfunction.m'. __Comments__ are given by __\\%__ An example would have the following contents.\n",
    "\n",
    " function y = squareThisNumber(x)\n",
    "\n",
    " y=x^2\n",
    "\n",
    " This tells us that there is one return value __y__ and one argument __x__. You can make a function that returns multiple values like this:\n",
    "\n",
    " function [y1,y2] = squareAndCubeThisNumber(x)\n",
    "\n",
    " y1=x^2;\n",
    "\n",
    " y2=x^3;\n",
    "\n",
    " * __Add path__ to find functions or other things with __addpath('pathToAdd')__.\n",
    "\n",
    " * __1-D Cost function__:\n",
    "\n",
    " function J = costFunctionJ(X, y, theta)\n",
    "\n",
    " m = size(X,1);\n",
    "\n",
    " predictions = X*theta;\n",
    " \n",
    " sqrErrors = (predictions-y).^2;\n",
    "\n",
    " J = 1/(2*m) * sum(sqrErrors);\n",
    "\n",
    "* __Vectorization__ - Keeping things simple and efficient\n",
    " \n",
    " Instead of trying write a routine to compute something like this:\n",
    "\n",
    " \\begin{equation*}\n",
    "    h_{\\theta}(x) = \\Sigma_{j=0}^n \\theta_j x_j\n",
    " \\end{equation*}\n",
    "\n",
    " think of it in terms of vectors like \n",
    "\n",
    " \\begin{equation*}\n",
    "    h_{\\theta}(x) = \\theta^T x\n",
    " \\end{equation*}\n",
    "\n",
    " so now you can use builtin efficient operations like __prediction = theta' * x__.\n",
    "\n",
    " * __Gradient Descent__: Vectors $\\theta$, and $\\delta$, scalar $\\alpha$, update all $\\theta_j$ values simultaneously with\n",
    "\n",
    " \\begin{equation*}\n",
    "    \\theta = \\theta - \\alpha \\delta \\textrm{, where } \\delta = \\frac{1}{m} \\Sigma_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}\n",
    " \\end{equation*}\n",
    "\n",
    " In practice, the __hypothesis__ $h_{\\theta}(x)$ is an $m$ x 1 vector formed by multiplying the $m$ x $(n+1)$ matrix $X$ with the $(n+1)$ x 1 vector $\\theta$. In this form we have the following (with no transposing)\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "   \\vec{x^{(1)}}\\\\\n",
    "   \\vec{x^{(2)}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\vec{x^{(m)}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = X\\theta = \n",
    "\\begin{bmatrix}\n",
    "   h_{\\theta}(x^{(1)})\\\\\n",
    "   h_{\\theta}(x^{(2)})\\\\\n",
    "   \\vdots\\\\\n",
    "   h_{\\theta}(x^{(m)})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    " Your errors vector is then an $m$ x 1 vector by just a subtraction\n",
    "\n",
    "$$\n",
    "E = h_{\\theta}(X) - y = \n",
    "\\begin{bmatrix}\n",
    "   h_{\\theta}(x^{(1)}) - y^{(1)}\\\\\n",
    "   h_{\\theta}(x^{(2)}) - y^{(2)}\\\\\n",
    "   \\vdots\\\\\n",
    "   h_{\\theta}(x^{(m)}) - y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    " Now you can actually finish off the entire summation and create an $(n+1)$ x 1 vector $\\delta$ by transposing your $X$ matrix and multiplying with the errors vector:\n",
    "\n",
    "$$\n",
    "\\delta = \\frac{1}{m} X^T E = \\frac{1}{m} \n",
    "\\begin{bmatrix}\n",
    "   \\vec{x^{(1)}} & \\vec{x^{(2)}} & \\dots & \\vec{x^{(m)}}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "   h_{\\theta}(x^{(1)}) - y^{(1)}\\\\\n",
    "   h_{\\theta}(x^{(2)}) - y^{(2)}\\\\\n",
    "   \\vdots\\\\\n",
    "   h_{\\theta}(x^{(m)}) - y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
