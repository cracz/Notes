{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning from Coursera by Andrew Ng\n",
    "\n",
    "---\n",
    "\n",
    "## Octave\n",
    " \n",
    " Useful for prototyping ML projects because it has ways to easily implement ML algorithms with builtin functions.\n",
    "\n",
    "--\n",
    " \n",
    "Examples of Machine Learning:\n",
    "\n",
    "* Database mining (Web click data, medical records, biology, engineering)\n",
    "* Applications that can't be programmed by hand (Autonomous helicopter, handwriting recognition, most of Natural Language Processing (NLP), computer vision.\n",
    "* Self-customizing programs (Amazon, Netflix product recommendations)\n",
    "\n",
    "## Machine Learning Algorithms\n",
    " \n",
    "### Supervised Learning\n",
    " \n",
    "* The \"correct answers\" are given for the algorithm to learn from; A.K.A. the \"training set\".\n",
    "* Usually useful for __Regression__: Predicting a continuous valued output.\n",
    "* __Classification__ problems: predicting a discrete valued output, like 0 or 1 (no or yes), specific categories, etc.\n",
    "* Notation for this course: $m$ = number of training examples, $x$ = input variable (features), $y$ = output variable (target variable), $(x,y)$ = single training example.\n",
    " \n",
    " \n",
    "### Unsupervised Learning\n",
    " \n",
    "* The \"correct answers\" are not given and the algorithm has to learn some structure in the data.\n",
    "* A common thing for the algorithms to do is to __cluster__ the data into separate groups. (i.e. Google News, human genes, organizing computing clusters, social network analysis, market segmentation, astronomical data analysis).\n",
    " \n",
    " \n",
    "### Univariate Linear Regression\n",
    " \n",
    "* One variable, model $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ is a straight line mapping $x$ to $y$ after fitting to the $m$ training examples.\n",
    "* Fitting process:\n",
    " * Find $\\theta_0$ and $\\theta_1$ in $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ such that we minimize the __cost function__ (or __squared error function__):\n",
    " \\begin{equation*}\n",
    "    J(\\theta_0, \\theta_1) = \\frac{1}{2m} * \\Sigma_{i=1}^m (h_{\\theta}(x_i)-y_i)^2\n",
    " \\end{equation*}\n",
    "\n",
    " \n",
    " ### Gradient Descent\n",
    "\n",
    "* Basic idea is to start with some $\\theta_0$, $\\theta_1$ and keep changing them to reduce $J(\\theta_0, \\theta_1)$ until we end up at a minimum.\n",
    "* This can be applied for any number of variables $\\theta_0,\\dots,\\theta_n$.\n",
    "* For each variable $\\theta_j$, repeatedly update $\\theta_j$ with $\\theta_j-(\\delta J/\\delta\\theta_j)*\\alpha$ (where $\\alpha$ is the __learning rate__).\n",
    "* You must simultaneously update all $\\theta_j$; don't do it one at a time because each update will affect the derivative for the other variables.\n",
    "\n",
    "\n",
    "### Multivariate Linear Regression\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
