{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning from Coursera by Andrew Ng\n",
    "\n",
    "---\n",
    "\n",
    "## Octave\n",
    " \n",
    " Useful for prototyping ML projects because it has ways to easily implement ML algorithms with builtin functions.\n",
    "\n",
    "--\n",
    " \n",
    "Examples of Machine Learning:\n",
    "\n",
    "* Database mining (Web click data, medical records, biology, engineering)\n",
    "* Applications that can't be programmed by hand (Autonomous helicopter, handwriting recognition, most of Natural Language Processing (NLP), computer vision.\n",
    "* Self-customizing programs (Amazon, Netflix product recommendations)\n",
    "\n",
    "## Machine Learning Algorithms\n",
    " \n",
    "### Supervised Learning\n",
    " \n",
    "* The \"correct answers\" are given for the algorithm to learn from; A.K.A. the \"training set\".\n",
    "* Usually useful for __Regression__: Predicting a continuous valued output.\n",
    "* __Classification__ problems: predicting a discrete valued output, like 0 or 1 (no or yes), specific categories, etc.\n",
    "* Notation for this course: $m$ = number of training examples, $x$ = input variable (features), $y$ = output variable (target variable), $(x,y)$ = single training example.\n",
    " \n",
    " \n",
    "### Unsupervised Learning\n",
    " \n",
    "* The \"correct answers\" are not given and the algorithm has to learn some structure in the data.\n",
    "* A common thing for the algorithms to do is to __cluster__ the data into separate groups. (i.e. Google News, human genes, organizing computing clusters, social network analysis, market segmentation, astronomical data analysis).\n",
    " \n",
    " \n",
    "### Univariate Linear Regression\n",
    " \n",
    "* One variable, model $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ is a straight line mapping $x$ to $y$ after fitting to the $m$ training examples.\n",
    "* Fitting process:\n",
    " * Find $\\theta_0$ and $\\theta_1$ in $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ such that we minimize the __cost function__ (or __squared error function__):\n",
    " \\begin{equation*}\n",
    "    J(\\theta_0, \\theta_1) = \\frac{1}{2m} * \\Sigma_{i=1}^m (h_{\\theta}(x_i)-y_i)^2\n",
    " \\end{equation*}\n",
    "\n",
    " \n",
    " ### Univariate Gradient Descent\n",
    "\n",
    " * Basic idea is to start with some $\\theta_0$, $\\theta_1$ and keep changing them to reduce $J(\\theta_0, \\theta_1)$ until we end up at a minimum.\n",
    " * This can be applied for any number of variables $\\theta_0,\\dots,\\theta_n$.\n",
    " * For each variable $\\theta_j$, repeatedly update $\\theta_j$ with $\\theta_j-(\\delta J/\\delta\\theta_j)*\\alpha$ (where $\\alpha$ is the __learning rate__).\n",
    " * You must simultaneously update all $\\theta_j$; don't do it one at a time because each update will affect the derivative for the other variables.\n",
    "\n",
    "\n",
    "### Multivariate Linear Regression\n",
    "\n",
    "* When there's more than one feature (independent variable) involved, the hypothesis becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "    h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\dots + \\theta_n x_n\n",
    "\\end{equation*}\n",
    "\n",
    "* Notation:\n",
    " * $x^{(i)} =$ the vector of features of the $i^{th}$ training example\n",
    " * $x_j^{(i)} =$ value of feature $j$ in the $i^{th}$ training example\n",
    " * $m = $ the number of training examples\n",
    " * $n = $ the number of features\n",
    "\n",
    "* Let $x_0^{(i)} = 1$ for all $i$. Then we can just do vector/matrix multiplication with two $n+1$ dimensional vectors $\\theta$ and $x$ so that \n",
    "\n",
    "\\begin{equation*}\n",
    "    h_{\\theta}(x) = \\theta^{T}x\n",
    "\\end{equation*}\n",
    "\n",
    "### Multivariate Gradient Descent\n",
    " \n",
    " * Use the same cost function, but now you have vectors $\\theta$, $x^{(i)}$, and $y^{(i)}$, and the derivatives are with respect to each $\\theta_j$:\n",
    "\n",
    " \\begin{equation*}\n",
    "    J(\\theta) = \\frac{1}{2m} * \\Sigma_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})^2 \\\\\n",
    "    \\theta_j = \\theta_j - \\alpha\\frac{\\delta J(\\theta)}{\\delta\\theta_j} \\textrm{ simultaneously for all } j = 0,\\dots,n\n",
    " \\end{equation*}\n",
    "\n",
    " * __Feature Scaling__: Make sure features are on a similar scale. This can help gradient descent converge quickly. Get every feature into approximately a $-1 \\leq x_i \\leq 1$ range. This is not an exact requirement, but is very useful.\n",
    "\n",
    " * __Mean Normalization__: Replace $x_i$ with $x_i-\\mu_i$ (where $\\mu_i$ is the average of $x_i$) to make features have approximately zero mean (do not apply to $x_0=1$). Or replace with $(x_i-\\mu_i)/s_i$, where $s_i$ is either the range of $x_i$ values or the standard deviation.\n",
    "\n",
    " * __Learning Rate__: Plot $J(\\theta)$ as gradient descent runs vs. number of iterations. Flattening out shows roughly how many iterations are necessary to converge. (example auto convergence test: declare convergence if $J(\\theta)$ decreases by less that $10^{-3}$ in one iteration.) If $J(\\theta)$ is increasing, or showing periodic behavior, use a smaller $\\alpha$. For sufficiently small $\\alpha$, gradient descent should decrease on __every__ iteration. An $\\alpha$ that is too small will make gradient descent very slow though. Maybe try 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.1.\n",
    "\n",
    " ### Features and Polynomial Regression\n",
    "\n",
    " * Rather than using an equation like $h_{\\theta}(x)=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3$, you may be able to choose a different feature $x$ that combines $x_1, x_2, x_3$ in some way so that you end up with a polynomial like $h_{\\theta}(x)=\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$. E.g. instead of using width and height, use area. The regression process is the same as before, but due to the exponents, feature scaling will become much more important.\n",
    "\n",
    "### Normal Equation\n",
    "\n",
    " * For a cost function $J(\\theta)$, the normal equation gives us a method to solve for $\\theta$ analytically, rather than using an iterative method like in gradient descent.\n",
    "\n",
    " * Usually to minimize a function $J(\\theta)$, we solve the equation $dJ(\\theta)/d\\theta=0$ for $\\theta$. If $\\theta$ is an $n+1$ dimensional vector, we solve the series of equations $\\delta J(\\theta)/\\delta\\theta_j=0$ for every $j$ to find $\\theta_0,\\theta_1,\\dots,\\theta_n$ that minimizes $J(\\theta)$.\n",
    "\n",
    " * for some dataset with $m$ training examples, and $n$ features ($x_1, x_2, \\dots, x_n$), we can add an $x_0$ feature of all 1's, and form a ``design matrix\" $X$ out of the values of all of the features. Each __row__ of $X$ is a training example, not columns. This will be an $m$ x $n+1$ matrix. Also we'll take the output values $y$ you're trying to predict and form an $m$-dimensional vector $Y$. The $\\theta$ that minimizes the cost function is given by $\\theta=(X^TX)^{-1}X^T Y$.\n",
    "\n",
    " * With this method, feature scaling is not necessary.\n",
    "\n",
    " * No need to choose $\\alpha$, don't need to iterate, however we need to compute $(X^TX)^{-1}$, an $n$ x $n$ matrix, and this makes the method slow for large $n$. The time to invert $X^TX$ grows like $O(n^3)$.\n",
    "\n",
    " * Maybe switch to gradient descent if $n\\sim 10^6$.\n",
    "\n",
    " * If $X^TX$ is non-invertible (singular or degenerate), it's usually because there are redundant features ($x_1=$ size in feet$^2$, $x_2=$ size in meters$^2$), or too many features ($m\\leq n$). If this invertibility problem arises, look at the features and get rid of redundant ones or use less features if there were too many, or use regularization.\n",
    "\n",
    "### Octave Tutorial\n",
    "\n",
    "* __Basic Operations__\n",
    "\n",
    " * Not equal is ~= and not !=. Pi is just given by \"pi\".\n",
    "\n",
    " * Single quotes to represent strings.\n",
    "\n",
    " * You can use the function __disp()__ to display things in the prompt, and use __sprintf()__ just like C __printf()__ to show strings with numbers.\n",
    "\n",
    " * Operations on or with matrices will automatically convert the necessary things like constants into matrices to properly perform the operations.\n",
    "\n",
    " * Create a matrix with square brackets and separate rows by semicolons. 3x2 matrix is A = [1 2; 3 4; 5 6]. To make a vector from 1 to 2 with step size of 0.1 between each element, use v = 1:0.1:2 (not semicolons). Or with step size of 1, just omit the middle number like v = 1:6.\n",
    "\n",
    " * To make a matrix of all ones, use the __function ones(rows, columns)__. Other functions include __zeros(rows, columns)__, __rand(rows, columns)__ that fills with random numbers between 0 and 1, and __randn(rows, columns)__ which uses Gaussian random numbers with mean = 0 and sigma = 1. \n",
    "\n",
    " * The function __hist(vector)__ plots a histogram with the data from the given vector. Or __hist(vector, bins)__ to get control of the number of bins.\n",
    "\n",
    " * __eye(n)__ gives an $n$x$n$ identity matrix.\n",
    "\n",
    " * Use __help functionName__ to get info on a particular function.\n",
    "\n",
    "* __Moving Data Around__\n",
    "\n",
    " * Use normal Unix command line commands to change/list directories, etc.\n",
    "\n",
    " * __load file.dat__ to load a certain text file called 'file.dat' that has training examples in rows and features/prediction values in columns. Make separate files for features and prediction values.\n",
    "\n",
    " * __who__ shows what variables are currently stored, __whos__ gives more details on the variables. __clear__ erases a variable you give it, or all variables if you don't give it any arguments.\n",
    "\n",
    " * Select first 10 elements of some vector __v__ with __v(1:10)__.\n",
    "\n",
    " * Use __save file.mat variable__ to save some variable like a matrix to a MATLAB type file called 'file.mat'. In order to save as human-readable text file, use __save file.txt variable -ascii__.\n",
    "\n",
    " * __size(matrix)__ returns a 1x2 vector containing the number of rows and number of columns. __size(matrix,1)__ gives just first element (rows), __size(matrix,2)__ gives just the second element (columns).\n",
    "\n",
    " * __length(matrix)__ returns the length of the longest dimension of the matrix. If it's a vector it'll just give you the length.\n",
    "\n",
    " * Retrieve the a,b element of some matrix A: __A(a,b)__. Retrieve everything in the ath row or bth column, use a colon like __A(a,:)__ or __A(:,b)__. The resulting vector will still be a row or column vector like it was in the matrix. To retrieve everything from the first and third rows at the same time, use __A([1 3],:)__. Use these same operations for assigning values within the matrix.\n",
    "\n",
    " * To append a new column to some matrix A, use __A = [A, [COLUMN VECTOR]]__. Putting two matrices A and B together is just __[A, B]__ (left and right) or __[A; B]__ (top and bottom).\n",
    "\n",
    " * __A(:)__ returns all the elements of matrix A rearranged into a 1-D vector.\n",
    "\n",
    "* __Computing on Data__\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
